{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "text"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        @font-face {\n",
       "            /* تعریف نام فونت */\n",
       "            font-family: 'BZar';\n",
       "            /* اکسپلورر 9 به بعد */\n",
       "            src: url('font/BZar.eot');\n",
       "            /* بررسی نصب بودن فونت در سیستم کاربر */\n",
       "            src: local('bZar'),\n",
       "                 /* برای برخی از مرورگرها مانند سافاری */\n",
       "                 local('b Zar'),\n",
       "                 /* هک برای اکسپلورر 8 و ماقبل */\n",
       "                 url('font/BZar.eot?#iefix') format('embedded-opentype'),\n",
       "                 /* فرمت مناسب مرورگرهای خیلی جدید */\n",
       "                 url('font/BZar.woff2') format('woff2'),\n",
       "                 /* فرمت مناسب مرورگرهای تقریبا جدید */\n",
       "                 url('font/BZar.woff') format('woff'),\n",
       "                 /* تمام مرورگرها به جزء اکسپلورر */\n",
       "                 url('font/BZar.ttf') format('truetype'),\n",
       "                 /* نسخه‌های قدیمی سیستم عامل iOS */\n",
       "                 url('font/BZar.svg#BZar') format('svg');\n",
       "            font-style: normal;\n",
       "            font-weight: normal;\n",
       "            font-display: swap;\n",
       "        }\n",
       "    \n",
       "    \n",
       "        .reveal .slides {\n",
       "            direction: rtl;\n",
       "            text-align: right;\n",
       "        }\n",
       "        \n",
       "        div {\n",
       "            direction: ltr;\n",
       "            text-align: left;\n",
       "        }\n",
       "        \n",
       "        p > img {\n",
       "          display: block;\n",
       "          margin-left: auto;\n",
       "          margin-right: auto;\n",
       "          max-width:75%; \n",
       "          height:auto;\n",
       "        }\n",
       "        \n",
       "        div.text_cell_render.rendered_html > *, li > p, .rendered_html p, #quarto-document-content > *\n",
       "        {\n",
       "            direction: rtl;\n",
       "            text-align: right;\n",
       "            font-family: BZar, Tahoma, Geneva, sans-serif;\n",
       "            font-size: x-large;\n",
       "            line-height: 26pt;\n",
       "        }\n",
       "        \n",
       "        .jp-CodeMirrorEditor .jp-Editor .jp-InputArea-editor {\n",
       "            direction: rtl !important;\n",
       "        }\n",
       "        \n",
       "        .CodeMirror-lines .output_subarea .output_text .output_stream .output_stdout{\n",
       "            direction: ltr !important;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "def f():\n",
    "    return HTML(\"\"\"\n",
    "    <style>\n",
    "        @font-face {\n",
    "            /* تعریف نام فونت */\n",
    "            font-family: 'BZar';\n",
    "            /* اکسپلورر 9 به بعد */\n",
    "            src: url('font/BZar.eot');\n",
    "            /* بررسی نصب بودن فونت در سیستم کاربر */\n",
    "            src: local('bZar'),\n",
    "                 /* برای برخی از مرورگرها مانند سافاری */\n",
    "                 local('b Zar'),\n",
    "                 /* هک برای اکسپلورر 8 و ماقبل */\n",
    "                 url('font/BZar.eot?#iefix') format('embedded-opentype'),\n",
    "                 /* فرمت مناسب مرورگرهای خیلی جدید */\n",
    "                 url('font/BZar.woff2') format('woff2'),\n",
    "                 /* فرمت مناسب مرورگرهای تقریبا جدید */\n",
    "                 url('font/BZar.woff') format('woff'),\n",
    "                 /* تمام مرورگرها به جزء اکسپلورر */\n",
    "                 url('font/BZar.ttf') format('truetype'),\n",
    "                 /* نسخه‌های قدیمی سیستم عامل iOS */\n",
    "                 url('font/BZar.svg#BZar') format('svg');\n",
    "            font-style: normal;\n",
    "            font-weight: normal;\n",
    "            font-display: swap;\n",
    "        }\n",
    "    \n",
    "    \n",
    "        .reveal .slides {\n",
    "            direction: rtl;\n",
    "            text-align: right;\n",
    "        }\n",
    "        \n",
    "        div {\n",
    "            direction: ltr;\n",
    "            text-align: left;\n",
    "        }\n",
    "        \n",
    "        p > img {\n",
    "          display: block;\n",
    "          margin-left: auto;\n",
    "          margin-right: auto;\n",
    "          max-width:75%; \n",
    "          height:auto;\n",
    "        }\n",
    "        \n",
    "        div.text_cell_render.rendered_html > *, li > p, .rendered_html p, #quarto-document-content > *\n",
    "        {\n",
    "            direction: rtl;\n",
    "            text-align: right;\n",
    "            font-family: BZar, Tahoma, Geneva, sans-serif;\n",
    "            font-size: x-large;\n",
    "            line-height: 26pt;\n",
    "        }\n",
    "        \n",
    "        .jp-CodeMirrorEditor .jp-Editor .jp-InputArea-editor {\n",
    "            direction: rtl !important;\n",
    "        }\n",
    "        \n",
    "        .CodeMirror-lines .output_subarea .output_text .output_stream .output_stdout{\n",
    "            direction: ltr !important;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\")\n",
    "\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <link rel=\"stylesheet\" href=\"css/jquery.jqZoom.css\" />\n",
       "    <script src=\"js/jquery-1.12.4.min.js\"></script>\n",
       "    <script src=\"js/jquery.zoom.min.js\"></script>\n",
       "    <script>\n",
       "        $(document).ready(function(){\n",
       "            $(\"img\").children().off();\n",
       "            $('img')\n",
       "            .wrap('<span style=\"display:inline-block\"></span>')\n",
       "            .css('display', 'block')\n",
       "            .parent()\n",
       "            .zoom({ on:'grab', duration: 150, magnify: 1 });\n",
       "        });\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "def g():\n",
    "    return HTML(\"\"\"\n",
    "    <link rel=\"stylesheet\" href=\"css/jquery.jqZoom.css\" />\n",
    "    <script src=\"js/jquery-1.12.4.min.js\"></script>\n",
    "    <script src=\"js/jquery.zoom.min.js\"></script>\n",
    "    <script>\n",
    "        $(document).ready(function(){\n",
    "            $(\"img\").children().off();\n",
    "            $('img')\n",
    "            .wrap('<span style=\"display:inline-block\"></span>')\n",
    "            .css('display', 'block')\n",
    "            .parent()\n",
    "            .zoom({ on:'grab', duration: 150, magnify: 1 });\n",
    "        });\n",
    "    </script>\n",
    "    \"\"\")\n",
    "\n",
    "g()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"فصل هشتم\"\n",
    "format: \n",
    "  docx:\n",
    "    toc: true\n",
    "    section-numbers: true\n",
    "    highlight-style: github\n",
    "    dir: rtl\n",
    "#  pptx:\n",
    "#    toc: true\n",
    "#    execute:\n",
    "#      echo: false\n",
    "#      warning: false\n",
    "  html:\n",
    "    css: css/style.css \n",
    "    code-fold: false \n",
    "    echo: true \n",
    "    self-contained: false \n",
    "    lang: fa\n",
    "#  revealjs: \n",
    "#    incremental: true\n",
    "#    dir: rtl \n",
    "#  pdf:\n",
    "#    number-sections: true\n",
    "#    colorlinks: true\n",
    "#    pdf-engine: xelatex\n",
    "#    keep-tex: true\n",
    "page-layout: full\n",
    "toc: false \n",
    "dir: rtl  \n",
    "lang: fa  \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# مقدمه ای بر یادگیری عمیق برای بینایی کامپیوتر\n",
    "\n",
    "**این فصل موارد زیر را پوشش می‌دهد**\n",
    "- \tآشنایی با شبکه‌های عصبی کانولوشنال (convnets)\n",
    "- \tاستفاده از افزایش داده‌ها برای کاهش بیش از حد برازش\n",
    "- \tاستفاده از یک convnet از پیش آموزش دیده برای انجام استخراج ویژگی\n",
    "- \tتنظیم دقیق یک شبکه از پیش آموزش دیده"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -\tبینایی کامپیوتر اولین و بزرگترین داستان موفقیت یادگیری عمیق است. \n",
    "        -\tGoogle Photos، \n",
    "        -\tجستجوی تصویر Google\n",
    "        -\tYouTube، \n",
    "        -\tفیلترهای ویدیویی در برنامه‌های دوربین، \n",
    "        -\tنرم‌افزار OCR\n",
    "        -\tرانندگی خودران، \n",
    "        -\tروباتیک، \n",
    "        -\tتشخیص پزشکی با کمک هوش مصنوعی، \n",
    "        -\tکشاورزی خودمختار autonomous farming\n",
    "    -\tبینایی کامپیوتری حوزه‌ای است که منجر به ظهور اولیه یادگیری عمیق بین سال‌های 2011 تا 2015 شد. \n",
    "    -\tبه طور قابل توجهی در پاییز 2012، گروه هینتون در چالش تشخیص تصویری در مقیاس بزرگ ImageNet برنده شد. \n",
    "    -\tجامعه پردازش تصویر در مقابل این پیشرفتها در ابتدا مقاومت می کردند!\n",
    "    -\tنوعی مدل یادگیری عمیق که اکنون تقریباً به طور جهانی در برنامه‌های بینایی رایانه استفاده می‌شود شبکه‌های کانولوشنی نامیده می‌شوند.\n",
    "    -\tدر این فصل استفاده از این شبکه‌ها برای مسائل دسته‌بندی روی مجموعه داده‌های کوچک بررسی می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مقدمه‌ای بر شبکه‌های عصبی کانولوشنی\n",
    "در فصل‌های قبل روی مسئله دسته‌بندی ارقام MNIST کار کردیم.\n",
    "- دقت دسته‌بندی با کمک شبکه عصبی کامل متصل برابر 97.8٪ بود.\n",
    "- در ادامه یک convnet ساده ایجاد می‌کنیم.\n",
    "\n",
    "**Instantiating a small convnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- یادآوری می‌شود که ورودی را برای دریافت یک تصویر تعریف می کنیم. \n",
    "- عدد ۱ برابر تعداد کانال هست که نشان‌دهنده تصاویر سیاه و سفید است.\n",
    "- بعدا می توانیم تصاویر را به صورت دسته‌ای تحویل دهیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نمایش summary مدل**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - می بینید که خروجی هر لایه Conv2D و MaxPooling2D یک تانسور شکل رتبه-3 است (ارتفاع، عرض، کانال). \n",
    "- ابعاد عرض و ارتفاع هر چه در مدل عمیق‌تر می‌شوید کاهش می‌یابد. \n",
    "- تعداد کانال‌ها توسط اولین آرگومان ارسال شده به لایه‌های Conv2D (32، 64 یا 128) کنترل می‌شود.\n",
    "- پس از آخرین لایه Conv2D، در نهایت با یک خروجی شکل (3، 3، 128) مواجه می‌شویم - نقشه ویژگی 3 × 3 از 128 کانال. \n",
    "- گام بعدی این است که این خروجی را به یک طبقه‌بندی‌کننده کاملا متصل  تغذیه کنید: \n",
    "- قبل از اضافه کردن لایه‌های متراکم، خروجی‌های سه بعدی را با یک لایه Flatten به یک بعدی صاف می‌کنیم.\n",
    "- در نهایت، ما طبقه بندی 10 طبقه‌ای را به کمک فعال‌ساز `softmax` می‌سازیم.\n",
    "- چون برچسب‌های ما اعداد صحیح هستند، ما از نسخه پراکنده، `sparse_categorical_crossentropy` استفاده کنید.\n",
    "- تعداد پارامتر‌ها از رابطه $(k^2N+1)C$ به دست می‌آید که $k$ اندازه کرنل و $N$ تعداد کانال ورودی و $C$ تعداد کانال خروجی است.\n",
    "- به نتیجه عمل کانولوشن در دو محور عرض و ارتفاع اصطلاحا نقشه ویژگی feature map می‌گویند.\n",
    "\n",
    "**Training the convnet on MNIST images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 18s 23ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0175 - val_accuracy: 0.9957\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 20s 27ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0125 - val_accuracy: 0.9973\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 19s 25ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0217 - val_accuracy: 0.9930\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 19s 25ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0245 - val_accuracy: 0.9942\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 19s 25ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.0358 - val_accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train, loss_val = history.history['loss'], history.history['val_loss']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_train, 'b')\n",
    "plt.plot(loss_val, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ارزیابی convnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 11ms/step - loss: 0.0821 - accuracy: 0.9852\n",
      "Test accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- در حالی که مدل با اتصال متراکم از فصل 2 دارای دقت تست 97.8٪ بود، convnet پایه دارای دقت تست 99.1٪ است: \n",
    "  -  میزان خطای نسبی را حدود 60٪ کاهش دادیم!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution operation\n",
    "\n",
    "تفاوت اساسی بین یک لایه متصل متراکم و یک لایه کانولوشن: \n",
    "- لایه‌های متراکم الگوهای سراسری را در فضای ویژگی ورودی خود یاد می‌گیرند (به عنوان مثال، برای یک رقم MNIST، الگوهای شامل همه پیکسل‌ها)\n",
    "- لایه‌های کانولوشن الگوهای محلی را یاد می‌گیرند. \n",
    "\n",
    "![تصاویر را می‌توان به الگوهای محلی مانند لبه ها، بافت‌ها و غیره تقسیم کرد.](img/08-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این مشخصه کلیدی دو ویژگی جالب به convnet‌ها می‌دهد:\n",
    "1. الگوهایی که آنها یاد می‌گیرند، تغییر ناپذیر `translation-invariant` هستند. \n",
    "   * پس از یادگیری یک الگوی خاص در گوشه پایین سمت راست یک تصویر، یک convnet می‌تواند آن را در هر جایی تشخیص دهد\n",
    "     * به عنوان مثال، در گوشه بالا سمت چپ. اگر یک مدل با اتصال متراکم در یک مکان جدید ظاهر شود، باید الگو را از نو یاد بگیرد. \n",
    "   * بنابراین شبکه‌های کانولوشنی به نمونه‌های آموزشی کمتری برای یادگیری بازنمایی‌هایی یا تعمیم بالا نیاز دارند.\n",
    "2. آنها می‌توانند سلسله مراتب فضایی  `spatial hierarchies of patterns` الگوها را بیاموزند. \n",
    "   * اولین لایه پیچشی الگوهای محلی کوچک مانند لبه‌ها را یاد می‌گیرد، \n",
    "   * لایه کانولوشن دوم الگوهای بزرگ‌تری را که از ویژگی‌های لایه‌های اول ساخته شده‌اند و غیره می‌آموزد \n",
    "   * این امر به شبکه‌های کانولوشنی اجازه می‌دهد تا مفاهیم بصری پیچیده و انتزاعی را به طور مؤثر یاد بگیرند، زیرا دنیای بصری اساساً از نظر فضایی سلسله مراتبی است."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![دنیای بصری یک سلسله مراتب فضایی از ماژول‌های بصری را تشکیل می‌دهد: خطوط یا بافت‌های ابتدایی به اشیاء ساده مانند چشم یا گوش ترکیب می‌شوند که در مفاهیم سطح بالا مانند \"گربه\" ترکیب می‌شوند.](img/08-02.png)\n",
    "\n",
    "![مفهوم نقشه پاسخ response map: یک نقشه 2 بعدی از حضور یک الگو در مکان‌های مختلف در ورودی](img/08-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- عملگر کانولوشن روی تکه‌های تصویر patch ها اعمال می‌شود و نقشه پاسخ تولید می‌کند.\n",
    "- پارامتر‌های مهم لایه‌ی کانولوشنی\n",
    "  - ابعاد کرنل\n",
    "  - تعداد کانال خروجی (عمق فضای ویژگی خروجی)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- یک کانولوشن با \n",
    "  - لغزاندن این پنجره‌های 3×3 یا 5×5 روی نقشه ویژگی ورودی سه‌بعدی، \n",
    "  - توقف در هر مکان ممکن، \n",
    "  - استخراج پچ سه‌بعدی ویژگی‌های اطراف کار می‌کند. \n",
    "- سپس هر یک از این وصله‌های سه‌بعدی به یک بردار یک بعدی به اندازه عمق خروجی تبدیل می‌شوند، \n",
    "  - این کار از طریق ضرب تنسور با یک ماتریس وزن، به نام هسته کانولوشن انجام می‌شود\n",
    "  - همان هسته در هر پچ دوباره استفاده می‌شود. \n",
    "- همه این بردارها (یکی در هر پچ) سپس به صورت مکانی در یک نقشه خروجی سه بعدی از شکل (ارتفاع، عرض، عمق ـ خروجی) جمع می‌شوند. \n",
    "- توجه داشته باشید که عرض و ارتفاع خروجی ممکن است به دو دلیل با عرض و ارتفاع ورودی متفاوت باشد:\n",
    "    - مرزها `borders`\n",
    "    - گام‌ها `strides`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![کانولوشن چطور کار می کند؟](img/08-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### درک تاثیر مرز و لایه‌گذاری (padding)\n",
    "- پس از اعمال عملگر کانولوشن نمی‌توانیم انتظار داشته باشیم تصویر نتیجه به همان اندازه ورودی باشد.\n",
    "  - یک نقشه ویژگی 5 × 5 (مجموع 25 کاشی) را در نظر بگیرید. فقط 9 کاشی وجود دارد که می‌توانید یک پنجره 3 × 3 را در اطراف آنها قرار دهید و یک شبکه 3 × 3 را تشکیل دهید.\n",
    "  - این حالت پیش‌فرض برای کانولوشن دو‌بعدی است که با نام `valid` خوانده می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![تاثیر مرز](img/08-05.png)\n",
    "\n",
    "- اگر بخواهیم همان ابعاد اولیه را دشته باشیم از لایه‌گذاری padding‌ استفاده می‌کنیم.\n",
    "  - برای کرنل ۳ در ۳ باید به سطر‌ها و ستون‌های تصویر اصلی دو ردیف پیکسلی اضافه کنیم.\n",
    "  - این حالت با پارامتر `padding=same` مشخص می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![لایه‌گذاری](img/08-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### درک مفهوم گام در کانولوشن convolution strides\n",
    "- گام میزان حرکت کرنل را مشخص می‌کند و از عوامل تاثیر گذار در اندازه خروجی است.\n",
    "- استفاده از گام 2 به این معنی است که عرض و ارتفاع نقشه ویژگی با ضریب 2 نمونه‌برداری می‌شود.\n",
    "  - این حالت در مسائل دسته‌بندی کاربرد کمی دارد.\n",
    "\n",
    "![گام پیچش برابر ۲](img/08-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### عملگر max-pooling\n",
    "- Max Pooling شامل استخراج پنجره‌ها از نقشه‌های ویژگی ورودی و خروجی حداکثر مقدار هر کانال است. \n",
    "- معمولا با گام ۲ برخلاف کانولوشن که ۳ در ۳ هست.\n",
    "- عدم استفاده از `max pooling` ۲ مشکل را در پی دارد:\n",
    "  1. برای یادگیری یک سلسله مراتب فضایی از ویژگی‌ها مناسب نیست. \n",
    "    - پنجره‌های 3 × 3 در لایه سوم فقط حاوی اطلاعاتی است که از 7 × 7 پنجره در ورودی اولیه می‌آید.\n",
    "    - الگوهای سطح بالایی که توسط convnet آموخته می‌شود با توجه به ورودی اولیه هنوز بسیار کوچک خواهد بود و برای یادگیری طبقه‌بندی ارقام احتمالا کافی نیست.\n",
    "      - (سعی کنید یک رقم را فقط با نگاه کردن به آن از طریق پنجره‌هایی با ابعاد 7 × 7 پیکسل تشخیص دهید! ).\n",
    "    - ما به ویژگی‌های آخرین لایه پیچیدگی نیاز داریم تا حاوی اطلاعاتی در مورد **کلیت ورودی** باشد.\n",
    "  2. بدون `max pooling` نقشه ویژگی دارای 61952  ضریب خواهد بود که همراه با لایه‌ی متراکم با اندازه ۱۰ حدود نیم‌میلیون پارامتر می‌شود!\n",
    "    - خطر بیش‌برازش"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An incorrectly structured convnet missing its max-pooling layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 22, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                619530    \n",
      "=================================================================\n",
      "Total params: 712,202\n",
      "Trainable params: 712,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- بنابراین مزیت استفاده از `max pooling` کاهش تعداد پارامترها و ایجاد امکان نگاه کلان به تصویر برای دسته‌بندی است.\n",
    "- می‌توان از `average pooling` هم استفاده کرد گرچه معمولا خیلی مفید نیست\n",
    "  - به حداکثر حضور ویژگی‌های مختلف نسبت به حضور متوسط آنها آموزنده‌تر است. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## آموزش یک convnet از ابتدا روی یک مجموعه داده کوچک\n",
    "- یکی از مسائل رایج دسته‌بندی آموزش دسته‌بندی روی یک مجموعه داده‌ی کوچک است.\n",
    "- به عنوان یک مثال عملی، ما بر طبقه‌بندی تصاویر به عنوان سگ یا گربه در مجموعه داده با مشخصات زیر تمرکز می‌کنیم.\n",
    "  - 5000 عکس از گربه و سگ (2500 گربه، 2500 سگ) \n",
    "    -   2000 تصویر برای آموزش، \n",
    "    -   1000 تصویر برای اعتبارسنجی \n",
    "    -   2000 تصویر برای آزمایش .\n",
    "- ابتدا آموزش را بدون چابک‌سازی انجام می‌دهیم تا به یک خط پایه برسیم.\n",
    "  - دقت ۷۰٪\n",
    "  - مسئله اصلی بیش‌برازش است.\n",
    "- سپس از تکنیک داده‌افزایی `data augmentation` برای کاهش مشکل بیش‌برازش استفاده می‌کنیم.\n",
    "  - دقت به حدود ۸۵٪ می‌رسد.\n",
    "- استخراج ویژگی از یک مدل از پیش‌آموزش دیده\n",
    "  - دقت ۹۷.۵٪\n",
    "- تنظیم دقیق یک مدل از پیش‌آموزش دیده\n",
    "  - دقت ۹۸.۵٪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نکته:** این سه استراتژی - آموزش یک مدل کوچک از ابتدا، انجام استخراج ویژگی با استفاده از یک مدل از پیش آموزش دیده، و تنظیم دقیق یک مدل از پیش آموزش دیده - جعبه ابزار آینده شما را برای حل مسائل طبقه‌بندی تصاویر با مجموعه داده‌های کوچک تشکیل می‌دهند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ارتباط یادگیری عمیق برای مسائل با مجموعه داده‌های کوچک\n",
    "با اینکه در مسئله مورد بحث تعداد داده‌های موجود کم هست، ولی بازهم شبکه‌های کانولوشنی نتایج قابل قبولی در پی دارند زیرا برای داده‌های حسب خوب کار می‌کنند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### دانلود مجموعه داده\n",
    "\n",
    "- دانلود از کگل نیازمند یکی از دو روش زیر است.\n",
    "  - ساخت حساب کاربری و دانلود از آدرس زیر\n",
    "    - [http://www.kaggle.com/c/dogs-vs-cats/data](www.kaggle.com/c/dogs-vs-cats/data)\n",
    "  - استفاده از `Kaggle API`\n",
    "    - In your account settings: \n",
    "      - API section => Create New API Token => ‍‍`kaggle.json` \n",
    "      - Upload it on `Colab` \n",
    "      - make sure that the file is only readable by the current user, yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- اولین باری که سعی می‌کنید داده‌ها را دانلود کنید، ممکن است با خطای \"403 Forbidden\" مواجه شوید.\n",
    "  - این به این دلیل است که قبل از دانلود مجموعه داده باید شرایط مرتبط با آن را بپذیرید \n",
    "  - باید به آن برویدwww.kaggle.com/c/dogs-vs-cats/rules(در حالی که به حساب Kaggle خود وارد شده اید) و روی دکمه I Understand and Accept کلیک کنید. فقط یک بار باید این کار را انجام دهید.\n",
    "  - \n",
    "- در نهایت، داده‌های آموزشی یک فایل فشرده با نام train.zip است.\n",
    "-  مطمئن شوید که آن را از حالت فشرده خارج کرده اید (از حالت فشرده خارج کنید) بی صدا (-qq):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!unzip -qq train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![نمونه تصاویر سگ و گربه](img/08-08.png)\n",
    "\n",
    "**کپی تصاویر به پوشه‌های training, validation و test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/cats_vs_dogs_small/train/cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m     15\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mcopyfile(src\u001b[38;5;241m=\u001b[39moriginal_dir \u001b[38;5;241m/\u001b[39m fname,\n\u001b[1;32m     16\u001b[0m                             dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m/\u001b[39m fname)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmake_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m make_subset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, end_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[1;32m     20\u001b[0m make_subset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, end_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mmake_subset\u001b[0;34m(subset_name, start_index, end_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m new_base_dir \u001b[38;5;241m/\u001b[39m subset_name \u001b[38;5;241m/\u001b[39m category\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     fnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_index, end_index)]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m fnames:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/cats_vs_dogs_small/train/cat'"
     ]
    }
   ],
   "source": [
    "# NOTE: this script is previously executed. You don't require to run it again\n",
    "\n",
    "import os, shutil, pathlib\n",
    "\n",
    "\n",
    "original_dir = pathlib.Path(\"/data/train\")  # unzip dogs-vs-cats.zip beforehand\n",
    "new_base_dir = pathlib.Path(\"data/cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ساخت مدل\n",
    "\n",
    "- ما مدل را با یک لایه Rescaling شروع می‌کنیم، که ورودی‌های تصویر (که مقادیر آنها در ابتدا در محدوده [0، 255] است) به محدوده [0، 1] تغییر مقیاس می‌دهد.\n",
    "- عمق نقشه‌های ویژگی به تدریج در مدل افزایش می‌یابد (از 32 به 256)، در حالی که اندازه نقشه‌های ویژگی کاهش می‌یابد (از 180 × 180 به 7 × 7). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating a small convnet for dogs vs. cats classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 178, 178, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 89, 89, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 87, 87, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 43, 43, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 41, 41, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 9, 9, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 12545     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 991,041\n",
      "Trainable params: 991,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**پیکربندی مدل برای آموزش**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### پیش‌پردازش داده‌ها\n",
    "مراحل زیر برای پیش‌پردازش تصاویر ورودی انجام می‌شود.\n",
    "\n",
    "1.\tفایل‌های تصویری را بخوانید.\n",
    "2.\tمحتوای JPEG را به شبکه‌های پیکسلی RGB رمزگشایی کنید.\n",
    "3.\tاینها را به تانسورهای ممیز شناور تبدیل کنید.\n",
    "4.\tاندازه آنها را به یک اندازه مشترک تغییر دهید (از 180 × 180 استفاده خواهیم کرد).\n",
    "5.\tآنها را به صورت دسته ای بسته بندی کنید (ما از دسته‌های 32 تصویری استفاده خواهیم کرد).\n",
    "\n",
    "\n",
    "**Using `image_dataset_from_directory` to read images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random_numbers = np.random.normal(size=(1000, 16))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "(16,)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- برای دسته بندی داده‌ها می‌توانیم از متد .batch() استفاده کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16)\n",
      "(32, 16)\n",
      "(32, 16)\n"
     ]
    }
   ],
   "source": [
    "batched_dataset = dataset.batch(32)\n",
    "for i, element in enumerate(batched_dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- از متد `map` برای تغییر شکل عناصر استفاده می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4, 4)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
    "for i, element in enumerate(reshaped_dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نمایش اشکال داده‌ها و برچسب‌های به دست آمده توسط مجموعه داده**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (32, 180, 180, 3)\n",
      "labels batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**برازش مدل**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 08:59:08.536969: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-25 08:59:09.238186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-25 08:59:10.223586: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-12-25 08:59:10.282534: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 51s 577ms/step - loss: 0.9133 - accuracy: 0.5010 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 19s 286ms/step - loss: 0.7248 - accuracy: 0.5408 - val_loss: 0.6828 - val_accuracy: 0.5370\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.7018 - accuracy: 0.5897 - val_loss: 0.6492 - val_accuracy: 0.6210\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.6631 - accuracy: 0.6078 - val_loss: 1.6920 - val_accuracy: 0.5060\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.6525 - accuracy: 0.6952 - val_loss: 0.6159 - val_accuracy: 0.6700\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.5693 - accuracy: 0.7087 - val_loss: 0.5751 - val_accuracy: 0.6980\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 20s 307ms/step - loss: 0.5583 - accuracy: 0.7362 - val_loss: 0.5831 - val_accuracy: 0.6870\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.4862 - accuracy: 0.7639 - val_loss: 0.6196 - val_accuracy: 0.6860\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 19s 291ms/step - loss: 0.4447 - accuracy: 0.8026 - val_loss: 0.6386 - val_accuracy: 0.6960\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.3901 - accuracy: 0.8221 - val_loss: 0.8965 - val_accuracy: 0.6670\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 19s 285ms/step - loss: 0.3477 - accuracy: 0.8357 - val_loss: 0.7356 - val_accuracy: 0.7210\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 19s 290ms/step - loss: 0.2737 - accuracy: 0.8830 - val_loss: 0.7769 - val_accuracy: 0.6990\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2179 - accuracy: 0.9024 - val_loss: 1.1915 - val_accuracy: 0.6900\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 19s 288ms/step - loss: 0.1928 - accuracy: 0.9208 - val_loss: 1.1448 - val_accuracy: 0.7060\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1510 - accuracy: 0.9396 - val_loss: 1.0331 - val_accuracy: 0.7040\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 19s 291ms/step - loss: 0.0972 - accuracy: 0.9628 - val_loss: 1.0647 - val_accuracy: 0.7220\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0829 - accuracy: 0.9742 - val_loss: 1.2100 - val_accuracy: 0.7130\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0865 - accuracy: 0.9665 - val_loss: 1.8315 - val_accuracy: 0.6720\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1060 - accuracy: 0.9603 - val_loss: 1.6352 - val_accuracy: 0.6840\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 19s 288ms/step - loss: 0.1152 - accuracy: 0.9639 - val_loss: 1.5247 - val_accuracy: 0.6840\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 1.8427 - val_accuracy: 0.7270\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0905 - accuracy: 0.9805 - val_loss: 1.7969 - val_accuracy: 0.7220\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0598 - accuracy: 0.9755 - val_loss: 1.8864 - val_accuracy: 0.7220\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0629 - accuracy: 0.9802 - val_loss: 1.9116 - val_accuracy: 0.7200\n",
      "Epoch 25/30\n",
      "29/63 [============>.................] - ETA: 7s - loss: 0.0552 - accuracy: 0.9807"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نمایش منحنی‌های loss و accuracy در طول آموزش**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ارزیابی مدل در مورد مجموعه داده تست**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### استفاده از افزونگی در داده‌ها  `data augmentation`\n",
    "\n",
    "- بیش‌برازش  به دلیل داشتن نمونه‌های بسیار کم برای یادگیری است که باعث می‌شود شما نتوانید مدلی را آموزش دهید که بتواند به داده‌های جدید تعمیم دهد.\n",
    "- اگر تعداد داده‌های ورودی کافی باشد و جنبه‌های ممکن تصویر ورودی را بتواند به درستی و کامل نشان دهد، مدل شما بیش‌برازش نمی‌کند.\n",
    "- تقویت داده‌ها رویکرد تولید داده‌های آموزشی بیشتر از نمونه‌های آموزشی موجود را با افزایش نمونه‌ها از طریق تعدادی تبدیل تصادفی دارد که تصاویری با ظاهر باورپذیر به دست می‌دهد.\n",
    "- هدف این است که، در زمان آموزش، مدل شما هرگز یک تصویر مشابه را دوبار نبیند. این \n",
    "  - تعمیم بهتر\n",
    "\n",
    "**تعریف یک مرحله افزایش داده برای افزودن به مدل تصویر**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نمایش تعدادی تصاویر تصادفی بعد از ایجاد افزونگی در داده‌های آموزشی**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- برای مقابله بیشتر با بیش‌برازش می‌توانیم از یک لایه `dropout` هم استفاده کنیم.\n",
    "\n",
    "**تعریف یک `convnet` جدید همراه با افزونگی در داده‌ها و `dropout`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- با توجه به استفاده از ابزارهایی برای چابک‌سازی مدل نیاز به تعداد بیشتری `epoch` برای بیش‌برازش داریم.\n",
    "\n",
    "**Training the regularized convnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ارزیابی مدل روی مجموعه تست**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "    \"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**نکته:** گرچه می‌توانیم بهبودهای بیشتری را در مدل بدهیم ولی دقت به زحمت بیش از ۹۰٪ خواهد شد!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## استفاده از یک مدل از پیش آموزش دیده\n",
    "- می‌توانید از یک مدل آموزش دیده روی مجموعه داده‌های بزرگ برای مدل‌سازی یک مسئله دسته‌بندی روی مجموعه داده‌ی کوچک استفاده کنید.\n",
    "- اگر این مجموعه داده اصلی به اندازه کافی بزرگ و کلی باشد، ویژگی‌های آن می‌تواند برای بسیاری از مسايل بینایی کامپیوتری مختلف مفید باشد، حتی اگر این مسائل جدید شامل کلاس‌های کاملاً متفاوتی نسبت به کارهای اصلی باشد.\n",
    "  - به عنوان مثال، می‌توانید یک مدل را در ImageNet آموزش دهید (که در آن کلاس‌ها عمدتاً حیوانات و اشیاء روزمره هستند) و سپس این مدل آموزش‌دیده را برای چیزی به دوردست‌تر از شناسایی وسایل مبلمان در تصاویر تغییر دهید.\n",
    "- در اینجا از معماری VGG16 (از ماژول `keras.applications`) که در سال 2014 بر روی ImageNet آموزش داده شده استفاده می‌کنیم.\n",
    "- دو راه برای استفاده از یک مدل از پیش آموزش دیده وجود دارد: \n",
    "1. استخراج ویژگی\n",
    "2. تنظیم دقیق"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### استخراج ویژگی با یک مدل از پیش آموزش دیده\n",
    "استخراج ویژگی شامل استفاده از نمایش‌های آموخته شده توسط یک مدل قبلا آموزش دیده برای استخراج ویژگی‌های جالب از نمونه‌های جدید است. سپس این ویژگی‌ها از طریق یک طبقه بندی کننده جدید اجرا می‌شوند که از ابتدا آموزش داده شده است.\n",
    "- قبلا دیدید که convnet شامل دو بخش زیر هست:\n",
    "- \tلایه‌های کانولوشنی و pooling است (پایه کانولوشن مدل برای استخراج ویژگی که عمومی است.)\n",
    "- \tیک طبقه‌بند تمام متصل (فاقد اطلاعات مکانی اشیا موجود در تصاویر - مربوط به مسئله خاص مورد بررسی ).\n",
    "\n",
    "![ تعویض طبقه‌بندی‌کننده‌ها با حفظ همان پایه کانولوشن](img/08-12.png)\n",
    "\n",
    "- لایه‌های اول شامل اطلاعات عام مانند لبه‌ها، رنگ‌ها و بافت‌ها هستند ولی لایه‌های بالاتر ویژگی‌های انتزاعی‌تری (مانند گوش گربه، چشم سگ و ...) را مدل می‌کنند. بنابراین با توجه به میزان تفاوت مسئله مورد بررسی با مدل آموزش‌دیده اولیه باید از تعداد لایه‌ی مناسب استفاده کرد. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating the VGG16 convolutional base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, # عدم گنجاندن طبقه‌بند کلی\n",
    "    input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- نقشه ویژگی نهایی دارای شکل (5، 5، 512) است. \n",
    "- این نقشه ویژگی است که در بالای آن لایه‌‌ی متراکم برای طبقه‌بندی‌ متصل می‌کنیم.\n",
    "- \tاستفاده از مدل آموزش‌دیده روی تصاویر و ذخیره نتایج روی دیسک و سپس اعمال لایه متراکم جدید (سرعت بالا – عدم امکان استفاده از افزونگی داده)\n",
    "- \tاتصال لایه متراکم به انتهای شبکه آموزش‌دیده و آموزش کل شبکه از اول (روش گران - دقت بالاتر)\n",
    "\n",
    "\n",
    "\n",
    "#### استخراج سریع ویژگی بدون افزایش داده ها\n",
    "بايد داده‌ها را پیش‌پردازش کنیم تا داده‌ها در مقیاس درست قرار گیرند.\n",
    "\n",
    "**Extracting the VGG16 features and corresponding labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mappend(labels)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(all_features), np\u001b[38;5;241m.\u001b[39mconcatenate(all_labels)\n\u001b[0;32m---> 13\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m  get_features_and_labels(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[1;32m     14\u001b[0m val_features, val_labels \u001b[38;5;241m=\u001b[39m  get_features_and_labels(validation_dataset)\n\u001b[1;32m     15\u001b[0m test_features, test_labels \u001b[38;5;241m=\u001b[39m  get_features_and_labels(test_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** تعریف و آموزش طبقه بندی کننده با اتصال متراکم **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آموزش بسیار سریع است زیرا ما فقط باید با دو لایه متراکم سر و کار داشته باشیم - یک دوره حتی در CPU کمتر از یک ثانیه طول می‌کشد.\n",
    "\n",
    "\n",
    "**رسم نتایج**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ معیارهای آموزش و اعتبارسنجی برای استخراج ویژگی ساده](img/08-13.png)\n",
    "\n",
    "- دقت اعتبارسنجی حدود ۹۷٪\n",
    "- علی‌رغم استفاده از `dropout` از همان ابتدا دچار بیش‌برازش شدیم، زیرا از افزونگی در داده استفاده نکردیم.\n",
    "\n",
    "\n",
    "#### استخراج ویژگی همراه با افزایش داده ها\n",
    "- روش دوم ایجاد مدلی است که `conv_base` را با یک طبقه‌بندی متراکم جدید زنجیره‌ای می‌کند و آموزش را به صورت `end2end` انجام می‌دهد. \n",
    "- بنابراین امکان افزونگی داده‌ای در ابتدای مدل وجود دارد.\n",
    "- بسیار کند و گران هست.\n",
    "- برای انجام این کار ابتدا پایه کانولوشن را فریز می‌کنیم:\n",
    "o جلوگیری از به روز شدن وزن آنها در طول آموزش است. \n",
    "o اگر این کار را نکنیم، بازنمایی‌هایی قبلاً آموخته شده از بین می‌روند.\n",
    "- در Keras، یک لایه یا مدل را با تنظیم ویژگی `trainable` آن بر روی `False` منجمد می‌کنیم.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**ایجاد و منجمد کردن پایه کانولوشنال VGG16 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_base  = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** چاپ لیست وزن‌های قابل آموزش قبل و بعد از انجماد **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights before freezing the conv base: 26\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "print(\"This is the number of trainable weights \"\n",
    "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights after freezing the conv base: 0\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "print(\"This is the number of trainable weights \"\n",
    "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** افزودن مرحله تقویت داده و یک طبقه‌بندی‌کننده به پایگاه کانولوشن **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "چون افزونگي داده‌اي ايجاد كرديم تعداد epoch هاي بيشتري را برای آموزش نياز داريم تا بیش‌برازش را ببینیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ارزیابی مدل در مجموعه تست **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "    \"feature_extraction_with_data_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "علی‌رغم هزینه‌ی زیادی که برای آموزش کردیم دقت افزایش آنچنانی نداشت (97.5٪)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تنظیم دقیق یک مدل از پیش آموزش دیده\n",
    "\n",
    "می‌توانیم چند لایه‌ی بالایی مدل پایه را (بجز batch normalization در صورت وجود) هم از حالت `freeze` خارج کنیم تا نمایش‌های انتزاعی مدل را بهبود دهد.\n",
    "یادآوری می‌شود که لایه‌های بالایی تخصصی‌تر هستند و نباید در unfreeze کردن لایه‌ها خیلی دست و دل‌باز باشیم وگرنه احتمالا با مشکل بیش‌برازش و حجم بالای وزن‌ها برای آموزش مواجه می‌شویم.\n",
    "\n",
    "![ تنظیم دقیق آخرین بلوک کانولوشنال شبکه VGG16](img/08-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freezing all layers until the fourth from the last**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما روی ۱۰٪ داده‌های مسابقه اصلی کگل به یکی از بهترین نتایج یعنی دقت 98.5٪ روی داده‌های تست رسیدیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## خلاصه\n",
    "-  Convnet‌ها بهترین نوع مدل‌های یادگیری ماشینی برای کارهای بینایی کامپیوتری هستند. آموزش از ابتدا حتی روی یک مجموعه داده بسیار کوچک، با نتایج مناسب امکان پذیر است.\n",
    "-  Convnet‌ها با یادگیری سلسله مراتبی از الگوها و مفاهیم پیمانه‌ای برای نمایش دنیای بصری کار می‌کنند.\n",
    "-  در یک مجموعه داده کوچک، مشکل اصلی بیش‌برازش است. \n",
    "-  برای داده‌های تصویری افزایش داده یک راه قدرتمند برای مبارزه با بیش‌برازش است.\n",
    "-  استفاده مجدد از یک convnet موجود در یک مجموعه داده جدید از طریق «استخراج ویژگی» آسان است. این یک تکنیک ارزشمند برای کار با مجموعه داده‌های تصویری کوچک است.\n",
    "-  به‌عنوان مکمل استخراج ویژگی، می‌توانید از «تنظیم دقیق» استفاده کنید، که برخی از نمایش‌هایی را که قبلاً توسط یک مدل موجود آموخته‌اند، با مسئله جدید تطبیق می‌دهد. البته این کار نیازمند کارآیی بیشتری است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
